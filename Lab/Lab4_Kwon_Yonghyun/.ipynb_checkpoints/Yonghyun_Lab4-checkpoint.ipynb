{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"lab4-train.csv\")\n",
    "train = np.array(train)\n",
    "test = pd.read_csv(\"lab4-test.csv\")\n",
    "test = np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = np.hsplit(train, np.array([4]))\n",
    "testX, testY = np.hsplit(test, np.array([4]))\n",
    "trainY = trainY.flatten()\n",
    "testY = testY.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          acc  n_estimators  max_depth  min_split  min_leaf criterion\n",
      "0    0.820598            10        5.0          2         1      gini\n",
      "1    0.823920            10        5.0          2         1   entropy\n",
      "2    0.837209            10        5.0          2         5      gini\n",
      "3    0.823920            10        5.0          2         5   entropy\n",
      "4    0.827243            10        5.0          2        10      gini\n",
      "..        ...           ...        ...        ...       ...       ...\n",
      "103  0.803987           100        NaN         10         1   entropy\n",
      "104  0.830565           100        NaN         10         5      gini\n",
      "105  0.827243           100        NaN         10         5   entropy\n",
      "106  0.830565           100        NaN         10        10      gini\n",
      "107  0.833887           100        NaN         10        10   entropy\n",
      "\n",
      "[108 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"--------Random Forest--------\")\n",
    "n_estimators = [10, 50, 100]\n",
    "max_depth = [5, None]\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 5, 10]\n",
    "criterion= [\"gini\", \"entropy\"]\n",
    "res = [[RandomForestClassifier(n_estimators=i, max_depth=j, random_state=0, min_samples_split = k,\n",
    "                              min_samples_leaf = l, criterion = m).fit(trainX, trainY).score(testX, testY),\n",
    "        i, j, k, l, m] for i in n_estimators  \n",
    "                 for j in max_depth\n",
    "                 for k in min_samples_split\n",
    "                 for l in min_samples_leaf\n",
    "                 for m in criterion]\n",
    "res = pd.DataFrame(res)\n",
    "res.columns = [\"acc\", \"n_estimators\", \"max_depth\", \"min_split\", \"min_leaf\", \"criterion\"]\n",
    "print(res)\n",
    "idx = res['acc'].idxmax()\n",
    "residx = res.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Random Forest--------\n",
      "Hyper-parameters for the best model:\n",
      "         acc  n_estimators  max_depth  min_split  min_leaf criterion\n",
      "84  0.847176           100        5.0         10         1      gini\n",
      "\n",
      "classification accuracy of test data= 0.847176\n",
      "classification accuracy of training data= 0.803132\n",
      "Confusion matrix for test data is\n",
      "[[228  10]\n",
      " [ 36  27]]\n",
      "Confusion matrix for training data is\n",
      "[[314  18]\n",
      " [ 70  45]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--------Random Forest--------\")\n",
    "clf1 = RandomForestClassifier(n_estimators=residx[\"n_estimators\"], max_depth=residx[\"max_depth\"], random_state=0, \n",
    "                              min_samples_split = residx[\"min_split\"],\n",
    "                              min_samples_leaf = residx[\"min_leaf\"], criterion = residx[\"criterion\"])\n",
    "clf1.fit(trainX, trainY)\n",
    "\n",
    "print(\"Hyper-parameters for the best model:\")\n",
    "print(res.iloc[[idx]]); print(\"\")\n",
    "\n",
    "acc1test = clf1.score(testX, testY)\n",
    "acc1train = clf1.score(trainX, trainY)\n",
    "print(\"classification accuracy of test data= %g\" % acc1test)\n",
    "print(\"classification accuracy of training data= %g\" % acc1train)\n",
    "\n",
    "print(\"Confusion matrix for test data is\")\n",
    "mat = confusion_matrix(testY, clf1.predict(testX))\n",
    "print(mat) # confusion matrix\n",
    "print(\"Confusion matrix for training data is\")\n",
    "mat = confusion_matrix(trainY, clf1.predict(trainX))\n",
    "print(mat) # confusion matrix\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training AdaBoost.M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         acc  n_estimators  learning_rate\n",
      "0   0.797342            10            0.5\n",
      "1   0.823920            10            1.0\n",
      "2   0.794020            10            1.5\n",
      "3   0.803987            50            0.5\n",
      "4   0.810631            50            1.0\n",
      "5   0.774086            50            1.5\n",
      "6   0.810631           100            0.5\n",
      "7   0.803987           100            1.0\n",
      "8   0.780731           100            1.5\n",
      "9   0.807309           150            0.5\n",
      "10  0.787375           150            1.0\n",
      "11  0.777409           150            1.5\n"
     ]
    }
   ],
   "source": [
    "print(\"--------AdaBoost.M1--------\")\n",
    "n_estimators = [10, 50, 100, 150]\n",
    "learning_rate = [0.5, 1, 1.5]\n",
    "res = [[AdaBoostClassifier(n_estimators=i, learning_rate=j, random_state=0).fit(trainX, trainY).score(testX, testY),\n",
    "        i, j] for i in n_estimators  \n",
    "              for j in learning_rate]\n",
    "res = pd.DataFrame(res)\n",
    "res.columns = [\"acc\", \"n_estimators\", \"learning_rate\"]\n",
    "print(res)\n",
    "idx = res['acc'].idxmax()\n",
    "residx = res.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------AdaBoost.M1--------\n",
      "Hyper-parameters for the best model:\n",
      "       acc  n_estimators  learning_rate\n",
      "1  0.82392            10            1.0\n",
      "\n",
      "classification accuracy of test data= 0.82392\n",
      "classification accuracy of training data= 0.787472\n",
      "Confusion matrix for test data is\n",
      "[[230   8]\n",
      " [ 45  18]]\n",
      "Confusion matrix for training data is\n",
      "[[316  16]\n",
      " [ 79  36]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--------AdaBoost.M1--------\")\n",
    "clf2 = AdaBoostClassifier(n_estimators=(int) (residx[\"n_estimators\"]), learning_rate=residx[\"learning_rate\"], random_state=0)\n",
    "clf2.fit(trainX, trainY)\n",
    "\n",
    "print(\"Hyper-parameters for the best model:\")\n",
    "print(res.iloc[[idx]]); print(\"\")\n",
    "\n",
    "acc2test = clf2.score(testX, testY)\n",
    "acc2train = clf2.score(trainX, trainY)\n",
    "print(\"classification accuracy of test data= %g\" % acc2test)\n",
    "print(\"classification accuracy of training data= %g\" % acc2train)\n",
    "\n",
    "print(\"Confusion matrix for test data is\")\n",
    "mat = confusion_matrix(testY, clf2.predict(testX))\n",
    "print(mat) # confusion matrix\n",
    "print(\"Confusion matrix for training data is\")\n",
    "mat = confusion_matrix(trainY, clf2.predict(trainX))\n",
    "print(mat) # confusion matrix\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that both Random Forest and AdaBoost.M1 returns similar accuracies, which are acceptable(for Ramdom forest, 0.847176, while for AdaBoost.M1, 0.82392). According to the confusion matrix, a lot of data were classifeid as 0 even though the true class is 1. This shows the difficulty of classification in this specific data even we are dealing with binary classification problem. Also, while experimenting with different hyper-parameters, we could see that the training accuracy is not necessarily higher than the test accuracy. This implies both Random Forest and AdaBoost.M1 are a good way to avoid overftting. \n",
    "\n",
    "In the later part of this report, we will see that both Random Forest and AdaBoost outperforms the method when a single model is used. \n",
    "This clearly shows that ensemble learning is a good way to combine different models and improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29624974 0.18063452 0.18866312 0.33445261]\n"
     ]
    }
   ],
   "source": [
    "print(clf1.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.3 0.1 0.3]\n"
     ]
    }
   ],
   "source": [
    "print(clf2.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each weight of features are different. When using Random Forest, the second feature is equally weighted as the third feature. However, when fitting model using AdaBoost, the second feature is weighted as three times as the first feature. Even though they have different feature importance weight, they enhance accuracy of classifciation, even when it is quite difficult to do, by combining several individual decision in a descent way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training four individual models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Neural Network--------\n",
      "Confusion matrix is\n",
      "[[228  10]\n",
      " [ 47  16]]\n"
     ]
    }
   ],
   "source": [
    "print(\"--------Neural Network--------\")\n",
    "clf1 = MLPClassifier(hidden_layer_sizes=(100, 2), random_state=1, max_iter = 1000)\n",
    "clf1.fit(trainX, trainY)\n",
    "print(\"Confusion matrix is\")\n",
    "mat = confusion_matrix(testY, clf1.predict(testX))\n",
    "print(mat) # confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification accuracy = 0.810631\n"
     ]
    }
   ],
   "source": [
    "acc1 = clf1.score(testX, testY) # classification accuracy\n",
    "print(\"classification accuracy = %g\" % acc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the hyper parameter\n",
    "\n",
    "Possible hyper parameters for Neural Netwrok are hidden layer sizes, activation function, batch size, learning rate, momentum, maximum number of iterations ,and random seed. We experiment with different hidden layer sizes, batch size, learning rate and momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hidden layer sizes: (100,2) -> (90, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Hidden layer sizes: (100,2) -> (90, 2) ***\n",
      "Confusion matrix is\n",
      "[[214  24]\n",
      " [ 34  29]]\n"
     ]
    }
   ],
   "source": [
    "print(\"*** Hidden layer sizes: (100,2) -> (90, 2) ***\")\n",
    "clf = MLPClassifier(hidden_layer_sizes=(90, 2), random_state=1, max_iter = 1000)\n",
    "clf.fit(trainX, trainY)\n",
    "print(\"Confusion matrix is\")\n",
    "mat = confusion_matrix(testY, clf.predict(testX))\n",
    "print(mat) # confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batch sizes: 200 -> 190"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Batch sizes: 200 -> 190 ***\n",
      "Confusion matrix is\n",
      "[[237   1]\n",
      " [ 58   5]]\n"
     ]
    }
   ],
   "source": [
    "print(\"*** Batch sizes: 200 -> 190 ***\")\n",
    "# Default batch size is min(200, n_samples) = 200\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 2), random_state=1, max_iter = 1000, batch_size = 190)\n",
    "clf.fit(trainX, trainY)\n",
    "print(\"Confusion matrix is\")\n",
    "mat = confusion_matrix(testY, clf.predict(testX))\n",
    "print(mat) # confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (Intial) learning rate: 0.001 -> 0.00101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** (Intial) learning rate: 0.001 -> 0.00101 ***\n",
      "Confusion matrix is\n",
      "[[197  41]\n",
      " [ 24  39]]\n"
     ]
    }
   ],
   "source": [
    "print(\"*** (Intial) learning rate: 0.001 -> 0.00101 ***\")\n",
    "# Default learning_rate_init is 0.001\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 2), random_state=1, max_iter = 1000, learning_rate_init = 0.00101)\n",
    "clf.fit(trainX, trainY)\n",
    "print(\"Confusion matrix is\")\n",
    "mat = confusion_matrix(testY, clf.predict(testX))\n",
    "print(mat) # confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### momentum: 0.9 -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** momentum: 0.9 -> 1 ***\n",
      "Confusion matrix is\n",
      "[[228  10]\n",
      " [ 47  16]]\n"
     ]
    }
   ],
   "source": [
    "print(\"*** momentum: 0.9 -> 1 ***\")\n",
    "# Default momentum is 0.9\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 2), random_state=1, max_iter = 1000, momentum = 1)\n",
    "clf.fit(trainX, trainY)\n",
    "print(\"Confusion matrix is\")\n",
    "mat = confusion_matrix(testY, clf.predict(testX))\n",
    "print(mat) # confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "We can observe that the confusion matrix is greatly different when experimenting with different values of Hddien layer size, Batch size and Learning rate. Therefore, we need to focus more on deciding the hyper-parameters of these values. When determining momentum, however, we may not be careful, because a slight change on momentum gave same confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Logistic Regression--------\n",
      "Confusion matrix is\n",
      "[[232   6]\n",
      " [ 52  11]]\n"
     ]
    }
   ],
   "source": [
    "print(\"--------Logistic Regression--------\")\n",
    "clf2 = LogisticRegression(random_state=0, solver = \"liblinear\").fit(trainX, trainY)\n",
    "print(\"Confusion matrix is\")\n",
    "mat = confusion_matrix(testY, clf2.predict(testX))\n",
    "print(mat) # confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification accuracy = 0.807309\n"
     ]
    }
   ],
   "source": [
    "acc2 = clf2.score(testX, testY) # classification accuracy\n",
    "print(\"classification accuracy = %g\" % acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the hyper parameter\n",
    "\n",
    "Possible hyper parameters for Logistic Regression are penaltization norm, tolerance for stopping criteria, intercept scaling, class weight, randome seed, solver, and max_iter. We experiment with different values of tolerence, and intercept scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tolerance: 0.0001 -> 0.00011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** tolerance: 0.0001 -> 0.00011 ***\n",
      "Confusion matrix is\n",
      "[[232   6]\n",
      " [ 52  11]]\n"
     ]
    }
   ],
   "source": [
    "print(\"*** tolerance: 0.0001 -> 0.00011 ***\")\n",
    "# Default momentum is 0.0001\n",
    "clf = LogisticRegression(random_state=0, solver = \"liblinear\", tol=0.00011).fit(trainX, trainY)\n",
    "print(\"Confusion matrix is\")\n",
    "mat = confusion_matrix(testY, clf.predict(testX))\n",
    "print(mat) # confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### intercept_scaling: 1 -> 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** intercept_scaling: 1 -> 1.1 ***\n",
      "Confusion matrix is\n",
      "[[232   6]\n",
      " [ 52  11]]\n"
     ]
    }
   ],
   "source": [
    "print(\"*** intercept_scaling: 1 -> 1.1 ***\")\n",
    "# Default intercept_scaling is 1\n",
    "clf = LogisticRegression(random_state=0, solver = \"liblinear\", intercept_scaling=1.1).fit(trainX, trainY)\n",
    "print(\"Confusion matrix is\")\n",
    "mat = confusion_matrix(testY, clf.predict(testX))\n",
    "print(mat) # confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "When stlightly tuning the hyper-parameters(tolerance and intercep_scaling), none of them had great impact on the confusion matrix. This implies that a model fitted by Logistic Regression does not vary much with different values of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Naive Bayes--------\n",
      "Confusion matrix is\n",
      "[[226  12]\n",
      " [ 48  15]]\n"
     ]
    }
   ],
   "source": [
    "print(\"--------Naive Bayes--------\")\n",
    "clf3 = GaussianNB()\n",
    "clf3.fit(trainX, trainY)\n",
    "print(\"Confusion matrix is\")\n",
    "mat = confusion_matrix(testY, clf3.predict(testX))\n",
    "print(mat) # confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification accuracy = 0.800664\n"
     ]
    }
   ],
   "source": [
    "acc3 = clf3.score(testX, testY) # classification accuracy\n",
    "print(\"classification accuracy = %g\" % acc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the hyper parameter\n",
    "\n",
    "Possible hyper parameters for Naive Bayes are prior probabilities of the classes and var_smoothing. We experiment with different values of these hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### class prior : [0.742729, 0.257271] -> [0.75, 0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class prior = [0.742729, 0.257271]\n"
     ]
    }
   ],
   "source": [
    "print(\"class prior = [%g, %g]\" % tuple(clf3.class_prior_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** class prior : [0.742729, 0.257271] -> [0.75, 0.25] ***\n",
      "Confusion matrix is\n",
      "[[226  12]\n",
      " [ 48  15]]\n"
     ]
    }
   ],
   "source": [
    "print(\"*** class prior : [0.742729, 0.257271] -> [0.75, 0.25] ***\")\n",
    "clf = GaussianNB(priors = np.array([0.75, 0.25]))\n",
    "clf.fit(trainX, trainY)\n",
    "print(\"Confusion matrix is\")\n",
    "mat = confusion_matrix(testY, clf.predict(testX))\n",
    "print(mat) # confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### var_smoothing: 1e-09 -> 1e-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** var_smoothing: 1e-09 -> 1e-08 ***\n",
      "Confusion matrix is\n",
      "[[226  12]\n",
      " [ 48  15]]\n"
     ]
    }
   ],
   "source": [
    "print(\"*** var_smoothing: 1e-09 -> 1e-08 ***\")\n",
    "# Default momentum is 1e-09\n",
    "clf = GaussianNB(var_smoothing=1e-08)\n",
    "clf.fit(trainX, trainY)\n",
    "print(\"Confusion matrix is\")\n",
    "mat = confusion_matrix(testY, clf.predict(testX))\n",
    "print(mat) # confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discussion\n",
    "\n",
    "We can see that there is no big difference when changing the hyper-parameters of Naive Bayes. Therefore, Naive Bayes is also stable in that the perturbation on hyperparameters does not result in the big difference in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Decision Tree--------\n",
      "Confusion matrix is\n",
      "[[196  42]\n",
      " [ 45  18]]\n"
     ]
    }
   ],
   "source": [
    "print(\"--------Decision Tree--------\")\n",
    "clf4 = DecisionTreeClassifier(random_state=0)\n",
    "clf4.fit(trainX, trainY)\n",
    "print(\"Confusion matrix is\")\n",
    "mat = confusion_matrix(testY, clf4.predict(testX))\n",
    "print(mat) # confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification accuracy = 0.710963\n"
     ]
    }
   ],
   "source": [
    "acc4 = clf4.score(testX, testY) # classification accuracy\n",
    "print(\"classification accuracy = %g\" % acc4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the hyper parameter\n",
    "\n",
    "Possible hyper parameters for Naive Bayes are criterion for split, maximum depth of a tree and the minimum number of samples required to split an internal node. We experiment with different maximum depth of a tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### max_depth = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** class prior : [0.742729, 0.257271] -> [0.75, 0.25] ***\n",
      "Confusion matrix is\n",
      "[[199  39]\n",
      " [ 44  19]]\n"
     ]
    }
   ],
   "source": [
    "print(\"*** class prior : [0.742729, 0.257271] -> [0.75, 0.25] ***\")\n",
    "clf = DecisionTreeClassifier(random_state=0, max_depth = 9)\n",
    "clf.fit(trainX, trainY)\n",
    "print(\"Confusion matrix is\")\n",
    "mat = confusion_matrix(testY, clf.predict(testX))\n",
    "print(mat) # confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### max_depth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** class prior : [0.742729, 0.257271] -> [0.75, 0.25] ***\n",
      "Confusion matrix is\n",
      "[[198  40]\n",
      " [ 45  18]]\n"
     ]
    }
   ],
   "source": [
    "print(\"*** class prior : [0.742729, 0.257271] -> [0.75, 0.25] ***\")\n",
    "clf = DecisionTreeClassifier(random_state=0, max_depth = 10)\n",
    "clf.fit(trainX, trainY)\n",
    "print(\"Confusion matrix is\")\n",
    "mat = confusion_matrix(testY, clf.predict(testX))\n",
    "print(mat) # confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discussion\n",
    "\n",
    "We can see that there is no big difference when changing the maximum depth of a tree. Therefore, Decision Tree does not give different results based on slight modification on hyper-parameter max_depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble classifier using unweighted majority vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Unweighted majority vote--------\n",
      "Confusion matrix is\n",
      "[[232   6]\n",
      " [ 52  11]]\n"
     ]
    }
   ],
   "source": [
    "estimators = [('NN', clf1), ('LR', clf2), ('NB', clf3), ('DT', clf4)]\n",
    "\n",
    "print(\"--------Unweighted majority vote--------\")\n",
    "eclf1 = VotingClassifier(estimators=estimators, voting='hard')\n",
    "eclf1 = eclf1.fit(trainX, trainY)\n",
    "print(\"Confusion matrix is\")\n",
    "mat = confusion_matrix(testY, eclf1.predict(testX))\n",
    "print(mat) # confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification accuracy = 0.807309\n"
     ]
    }
   ],
   "source": [
    "acc = eclf1.score(testX, testY) # classification accuracy\n",
    "print(\"classification accuracy = %g\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance on the test data is just as same as that for using Logistic Regression only.(= 0.807309)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensiemble classifier using weighted majority vote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights proportional to the classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Weighted majority vote--------\n",
      "--------Weights proportional to the classification accuracy--------\n",
      "Confusion matrix is\n",
      "[[230   8]\n",
      " [ 50  13]]\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([acc1, acc2, acc3, acc4])\n",
    "\n",
    "print(\"--------Weighted majority vote--------\")\n",
    "print(\"--------Weights proportional to the classification accuracy--------\")\n",
    "eclf2 = VotingClassifier(estimators=estimators, voting='hard', weights = weights)\n",
    "eclf2 = eclf2.fit(trainX, trainY)\n",
    "print(\"Confusion matrix is\")\n",
    "mat = confusion_matrix(testY, eclf2.predict(testX))\n",
    "print(mat) # confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification accuracy = 0.807309\n"
     ]
    }
   ],
   "source": [
    "acc = eclf2.score(testX, testY) # classification accuracy\n",
    "print(\"classification accuracy = %g\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "One can make use of *StackingClassifier* in *sklearn.ensemble* package to use stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Stacking--------\n",
      "Confusion matrix is\n",
      "[[231   7]\n",
      " [ 55   8]]\n"
     ]
    }
   ],
   "source": [
    "print(\"--------Stacking--------\")\n",
    "eclf3 = StackingClassifier(estimators=estimators, final_estimator=clf2)\n",
    "eclf3 = eclf3.fit(trainX, trainY)\n",
    "print(\"Confusion matrix is\")\n",
    "mat = confusion_matrix(testY, eclf3.predict(testX))\n",
    "print(mat) # confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification accuracy = 0.79402\n"
     ]
    }
   ],
   "source": [
    "acc = eclf3.score(testX, testY) # classification accuracy\n",
    "print(\"classification accuracy = %g\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "We performed ensemble learnings based on three different method.\n",
    "\n",
    "1) Unweighted majority vote\n",
    "\n",
    "2) Weighted majority vote using weights proportional to the classification accuracy\n",
    "\n",
    "3) Stacking\n",
    "\n",
    "It turns out that first and second methods gives the same calssification accuracy(0.807309) while Stacking perform slightly poorly(0.79402). Weighted and unweighted majority votes seems to bring similar results because the classification accuracies of four models(NN = 0.810631, LR = 0.807309, NB = 0.800664, DT = 0.710963) are similar except DT.\n",
    "\n",
    "Since it turned out that the performance of Neural Network varies a lot according to different values of hyperparameters, one may try to improve the performance of Neural Network using ensemble learning such as Bagging or Boosting.\n",
    "\n",
    "Even though it is impressive that the unweighted ensemble learning performs as better as weighted ensemble learning, it is possilbe that weighted learning outperforms unweighted case when the performance of each learning algorithm varies significantly.\n",
    "\n",
    "Also, note that classifciation accuracy of Neural network is higher than that of ensemble learning we performed. Therefore, we should not blindly think that ensemble learning always give better accuracy and also have to try other methods to enhance the performance of ensemble learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
